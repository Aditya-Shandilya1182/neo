{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP6Rsh7dxd7vAqVQaAGEkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-Shandilya1182/neo/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7AxBPoZ49aN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#embd -> DIM\n",
        "#hidden_dim -> FFN_DIM\n",
        "#n_head -> N_HEADS\n",
        "#head_dim -> HEAD_DIM"
      ],
      "metadata": {
        "id": "90-poBJlCWTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim, norm_eps):\n",
        "    super().__init__()\n",
        "    self.norm_eps = norm_eps\n",
        "    self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def _norm(self, x):\n",
        "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self._norm(x.float()).type_as(x)\n",
        "    return out * self.weight"
      ],
      "metadata": {
        "id": "5IK90rfGjamh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.w1 = nn.Linear(config.embd, config.hidden_dim, bias=False)\n",
        "    self.w2 = nn.Linear(config.hidden_dim, config.embd, bias=False)\n",
        "    self.w3 = nn.Linear(config.embd, config.hidden_dim, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_w1 = self.w1(x)\n",
        "    x_w3 = self.w3(x)\n",
        "    x = F.silu(x_w1) * x_w3\n",
        "    x = self.w2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "P9UZXnH15zGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "def precompute_freqs_cis(dim, end, theta = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis, x):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)"
      ],
      "metadata": {
        "id": "4fCwlEArjZbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.wq = nn.Linear(config.embd, config.n_heads * config.head_dim, bias=False)\n",
        "    self.wk = nn.Linear(config.embd, config.n_kv_head * config.head_dim, bias=False)\n",
        "    self.wv = nn.Linear(config.embd, config.n_kv_head * config.head_dim, bias=False)\n",
        "    self.wo = nn.Linear(config.n_heads * config.head_dim, config.embd, bias=False)\n",
        "\n",
        "    self.cache_k = torch.zeros((config.batch_size, config.seq_len, config.n_kv_head, config.head_dim))\n",
        "    self.cache_v = torch.zeros((config.batch_size, config.seq_len, config.n_kv_head, config.head_dim))\n",
        "\n",
        "    self.n_heads = config.n_heads\n",
        "    self.head_dim = config.head_dim\n",
        "    self.n_kv_head = config.n_kv_head\n",
        "    self.n_kv_head_rep = config.n_kv_head_rep\n",
        "\n",
        "  def forward(self, x, start_pos, freqs_cis, mask):\n",
        "    batch_sz, seqlen, _ = x.shape\n",
        "\n",
        "    queries = self.wq(x)\n",
        "    keys = self.wk(x)\n",
        "    values = self.wv(x)\n",
        "\n",
        "    queries = queries.view(batch_sz, seqlen, self.n_heads, self.head_dim)\n",
        "    keys = keys.view(batch_sz, seqlen, self.n_kv_head, self.head_dim)\n",
        "    values = values.view(batch_sz, seqlen, self.n_kv_head, self.head_dim)\n",
        "\n",
        "    queries, keys = apply_rotary_emb(queries, keys, freqs_cis)\n",
        "\n",
        "    self.cache_k = self.cache_k.to(queries.device)\n",
        "    self.cache_v = self.cache_v.to(queries.device)\n",
        "\n",
        "    self.cache_k[:batch_sz, start_pos : start_pos + seqlen] = keys\n",
        "    self.cache_v[:batch_sz, start_pos : start_pos + seqlen] = values\n",
        "\n",
        "    keys = self.cache_k[:batch_sz, : start_pos + seqlen]\n",
        "    values = self.cache_v[:batch_sz, : start_pos + seqlen]\n",
        "\n",
        "    keys = torch.repeat_interleave(keys, dim=2, repeats=self.n_kv_head_rep)\n",
        "    values = torch.repeat_interleave(values, dim=2, repeats=self.n_kv_head_rep)\n",
        "\n",
        "    queries = queries.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    out = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask)\n",
        "    out = out.transpose(1, 2).contiguous().view(batch_sz, seqlen, -1)\n",
        "    return self.wo(out)"
      ],
      "metadata": {
        "id": "ImlpMLst9yN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.attention = Attention(config)\n",
        "    self.feed_forward = MLP(config)\n",
        "    self.attention_norm = RMSNorm(config.embd, config.norm_eps)\n",
        "    self.ffn_norm = RMSNorm(config.embd, config.norm_eps)\n",
        "\n",
        "  def forward(self, x, start_pos, freqs_cis, mask):\n",
        "     x = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
        "     x = x + self.feed_forward(self.ffn_norm(x))\n",
        "     return x"
      ],
      "metadata": {
        "id": "JH-ylukPoXTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neo(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.tok_embeddings = nn.Embedding(config.vocab_size, config.embd)\n",
        "    self.layers = nn.ModuleList()\n",
        "    for _ in range(config.n_layers):\n",
        "      self.layers.append(Block(config))\n",
        "    self.norm = RMSNorm(config.embd, config.norm_eps)\n",
        "    self.output = nn.Linear(config.embd, config.vocab_size, bias=False)\n",
        "    self.freq_cis = precompute_freqs_cis(config.head_dim, config.seq_len * 2, config.theta)\n",
        "\n",
        "  @torch.inference_mode()\n",
        "  def forward(self, tokens, start_pos):\n",
        "    bsz, seqlen = tokens.shape\n",
        "    h = self.tok_embeddings(tokens)\n",
        "    self.freqs_cis = self.freqs_cis.to(tokens.device)\n",
        "    freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
        "    mask = None\n",
        "    if seqlen > 1:\n",
        "      mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
        "\n",
        "      mask = torch.triu(mask, diagonal=1).to(tokens.device)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_cis, mask)\n",
        "    h = self.norm(h)\n",
        "    out = self.output(h).float()\n",
        "    return out"
      ],
      "metadata": {
        "id": "K9Jyp1mVktnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "  embd: int = 4096\n",
        "  hidden_dim: int = 14336\n",
        "  n_heads: int = 32\n",
        "  head_dim: int = embd // n_heads\n",
        "  n_kv_heads: int = 8\n",
        "  vocab_size: int = 128256\n",
        "  n_layers: int = 32\n",
        "  norm_eps: int = 1e-5\n",
        "  theta: int = 500000\n",
        "  seq_len: int = 128\n",
        "  n_kv_heads_rep: int = n_heads // n_kv_heads"
      ],
      "metadata": {
        "id": "EMnodrbHm-yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelConfig()\n",
        "model = Neo(config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "j5XOzUFXqGLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, epochs, learning_rate, device, compile_model=False):\n",
        "  model.to(device)\n",
        "  if compile_model:\n",
        "    model = torch.compile(model)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch, start_pos = 0)\n",
        "        loss = criterion(output[:, :-1].reshape(-1, config.vocab_size), batch[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "      avg_loss = total_loss/ len(dataloader)\n",
        "      print(f\"Epoch: {epoch + 1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "iGRJMoVqze6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}